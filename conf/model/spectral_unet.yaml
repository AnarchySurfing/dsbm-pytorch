# @package _global_

# Spectral U-Net Model Configuration
# Optimized for visible-to-infrared image translation tasks

Model: UNET
model: 
    # Base architecture parameters
    num_channels: 128  # Base number of channels
    channel_mult: [1, 2, 2, 4, 4]  # Channel multipliers for each resolution level
    num_res_blocks: 3  # Residual blocks per resolution level
    num_heads: 8  # Attention heads
    attention_resolutions: "32,16,8"  # Resolutions where attention is applied
    dropout: 0.1  # Dropout rate
    use_checkpoint: false  # Gradient checkpointing (enable for memory saving)
    use_scale_shift_norm: true  # Scale-shift normalization
    resblock_updown: true  # Use residual connections in up/downsampling
    temb_scale: 1000  # Time embedding scale
    
    # Spectral-specific parameters
    spectral:
        # Cross-spectral attention mechanisms
        enable_cross_spectral_attention: true
        cross_attention_layers: [2, 3, 4]  # Layers with cross-spectral attention
        spectral_embedding_dim: 64  # Spectral feature embedding dimension
        
        # Wavelength-aware processing
        wavelength_conditioning: true
        wavelength_embed_dim: 32
        temperature_conditioning: true  # For thermal infrared
        temperature_embed_dim: 16
        
        # Multi-scale spectral features
        enable_spectral_pyramid: true
        pyramid_levels: 3
        spectral_fusion_method: "attention"  # "concat", "add", "attention"
        
        # Spectral consistency mechanisms
        spectral_consistency_loss: true
        consistency_weight: 0.1
        gradient_consistency: true
        structural_consistency: true
        
        # Advanced spectral features
        enable_frequency_domain_processing: true
        fft_channels: 32  # Channels for frequency domain processing
        spectral_normalization: "adaptive"  # "batch", "layer", "adaptive"
        
    # Architecture optimizations for spectral data
    architecture:
        # Efficient attention for high-resolution images
        attention_type: "linear"  # "full", "linear", "sparse"
        attention_dropout: 0.1
        
        # Residual connections
        residual_scaling: 0.1  # Scale residual connections
        skip_connection_scaling: 0.2  # Scale skip connections
        
        # Normalization
        norm_type: "group"  # "batch", "layer", "group", "adaptive"
        num_groups: 32  # For group normalization
        
        # Activation functions
        activation: "swish"  # "relu", "gelu", "swish"
        
    # Training optimizations
    training:
        # Mixed precision training
        mixed_precision: true
        
        # Gradient optimization
        gradient_clipping: true
        max_grad_norm: 1.0
        
        # Memory optimization
        memory_efficient: true
        checkpoint_segments: 4  # Number of checkpoint segments
        
        # Spectral loss components
        loss_weights:
            reconstruction: 1.0
            spectral_consistency: 0.1
            perceptual: 0.1
            adversarial: 0.01  # If using adversarial training
            
    # Inference optimizations
    inference:
        # Sampling optimization
        enable_fast_sampling: true
        ddim_eta: 0.0  # DDIM sampling parameter
        
        # Memory optimization during inference
        low_memory_mode: false
        tile_size: 512  # For tiled inference of large images
        tile_overlap: 64  # Overlap between tiles
        
        # Quality vs speed trade-offs
        quality_mode: "high"  # "fast", "balanced", "high"
        
# Model size variants (can be overridden)
variants:
    small:
        num_channels: 64
        channel_mult: [1, 2, 2, 4]
        num_res_blocks: 2
        attention_resolutions: "16,8"
        
    medium:
        num_channels: 128
        channel_mult: [1, 2, 2, 4, 4]
        num_res_blocks: 3
        attention_resolutions: "32,16,8"
        
    large:
        num_channels: 192
        channel_mult: [1, 1, 2, 2, 4, 4]
        num_res_blocks: 4
        attention_resolutions: "64,32,16,8"
        
    xlarge:
        num_channels: 256
        channel_mult: [1, 1, 2, 2, 4, 4, 8]
        num_res_blocks: 4
        attention_resolutions: "64,32,16,8"

# Hardware-specific optimizations
hardware:
    # GPU optimizations
    gpu:
        enable_tf32: true  # Enable TF32 on Ampere GPUs
        enable_flash_attention: true  # Use flash attention if available
        memory_format: "channels_last"  # Memory layout optimization
        
    # Multi-GPU settings
    distributed:
        backend: "nccl"
        find_unused_parameters: false
        gradient_as_bucket_view: true